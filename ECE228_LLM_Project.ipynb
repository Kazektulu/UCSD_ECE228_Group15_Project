{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d502d803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerate version: 0.21.0\n",
      "Peft version: 0.4.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "#from trl import SFTTrainer\n",
    "#from peft import LoraConfig, PeftModel\n",
    "#import trl\n",
    "from trl import SFTTrainer\n",
    "import peft\n",
    "from peft import LoraConfig\n",
    "import accelerate \n",
    "import bitsandbytes\n",
    "import transformers\n",
    "\n",
    "print(\"Accelerate version:\", accelerate.__version__)\n",
    "print(\"Peft version:\", peft.__version__)\n",
    "#print(\"BitsAndBytes version:\", bitsandbytes.__version__)\n",
    "#print(\"Transformers version:\", transformersformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9107f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n",
    "# Further package issues:: tried various things. some worked, some not -- will find workaround.\n",
    "\n",
    "#bin_path = \"/home/ssharp/.local/bin\"\n",
    "#os.environ['PATH'] = f\"{bin_path}:{os.environ['PATH']}\"\n",
    "#sys.path.append(bin_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2b206cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model to train, based on HuggingFace naming-conventions\n",
    "# Originally selected model incompliant - no available config file to feed the transformers wrapper\n",
    "#model_name = \"aptha/Meta-Llama-3-8B-Instruct-Q4_0-GGUF\"\n",
    "model_name = \"NousResearch/llama-2-7b-chat-hf\"\n",
    "\n",
    "# The instruction dataset to use:\n",
    "dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "#new_model = \"llama-3-Llamaguaco\"\n",
    "new_model = \"llama-2-mamma\"\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.2\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading:\n",
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_nested_quant = True\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "#output_dir = \"./Llama_3_results\"\n",
    "output_dir = \"model_results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 3\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 1\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 1\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 5\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule (constant a bit better than cosine)\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 100\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "#max_seq_length = None\n",
    "max_seq_length = 8192\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f765c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdd15927",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b37728f768f546698520a062ab6c4e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(dataset_name, cache_dir=None, split=\"train\")\n",
    "\n",
    "# Activate 4-bit precision base model loading:\n",
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_nested_quant = True\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "#bnb_config = BitsAndBytesConfig(\n",
    "#    load_in_4bit=use_4bit,\n",
    "#    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "#    bnb_4bit_compute_dtype=compute_dtype,\n",
    "#    bnb_4bit_use_double_quant=use_nested_quant,\n",
    "#)\n",
    "\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8: \n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Load base model\n",
    "#model = AutoModelForCausalLM.from_pretrained(\n",
    "#    model_name,\n",
    "#    quantization_config=bnb_config,\n",
    "#    device_map=device_map\n",
    "#)\n",
    "\n",
    "\n",
    "max_memory = {i: '12000MB' for i in range(torch.cuda.device_count())}\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = use_4bit,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type='nf4'\n",
    "    )\n",
    "\n",
    "# AutoModel selects most appropriate model configuration for -model_name-\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    #\"aptha/Meta-Llama-3-8B-Instruct-Q4_0-GGUF/blob/main/Meta-Llama-3-8B-Instruct-Q4_0.gguf\",\n",
    "    cache_dir=None,\n",
    "    #load_in_4bit=True,\n",
    "    device_map='auto',\n",
    "    max_memory=max_memory,\n",
    "    #torch_dtype=torch.bfloat16,\n",
    "    quantization_config=quant_config\n",
    ")\n",
    "\n",
    "# Load LLaMA tokenizer for use with AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=None, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "704a2d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssharp/.local/lib/python3.9/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42228ab913b64b39b07ccde7f98201b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45cf521",
   "metadata": {},
   "source": [
    "# Saving Trained Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81a986e1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "############ Train & Save Model  ####################\n",
    "\n",
    "#trainer.train()\n",
    "#trainer.model.save_pretrained(new_model)\n",
    "\n",
    "#### Comment out to prevent accidentally over-writing saved model. ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c94cbd",
   "metadata": {},
   "source": [
    "# This is a checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c970689",
   "metadata": {},
   "source": [
    "### Post-training should be defined below: Load a model from local memory then run inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "461c90d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e980bf0246b54b6d87d5867376019b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 6,771,970,048 || trainable%: 0.0\n"
     ]
    }
   ],
   "source": [
    "#test_model = trainer.model.load_pretrained\n",
    "\n",
    "#model_path = \"/\"\n",
    "\n",
    "# Load the model\n",
    "\n",
    "#config = transformers.AutoConfig.from_pretrained(\"./llama-2-mamma/\")\n",
    "#loaded_model = AutoModelForCausalLM.from_pretrained(\"./llama-2-mamma/\")\n",
    "\n",
    "\n",
    "# Loading Quantized Model is different per each wrapper distortion --\n",
    "# using peft: load Quantized FineTune_Model = Base_model + Tuned Adapter : still need to load base model seperately\n",
    "\n",
    "#from peft import PeftModel, PeftConfig\n",
    "import torch\n",
    "import peft\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from accelerate import infer_auto_device_map, init_empty_weights\n",
    "\n",
    "#model_name = \"NousResearch/llama-2-7b-chat-hf\" # Base Model to test\n",
    "#base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "peft_model_name = \"./llama-2-mamma/\"\n",
    "\n",
    "#saved info on fine-tune includes reference to base_model name\n",
    "config = peft.PeftConfig.from_pretrained(peft_model_name)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    torch_dtype='auto',\n",
    "    device_map='auto',\n",
    "    offload_folder=\"offload\", offload_state_dict = True\n",
    ")\n",
    "\n",
    "# Tokenizer is not saved automatically : will just re-define the previous definition since it doesnt change.\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, cache_dir=None, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix for an overflow issue with the fp16 training\n",
    "\n",
    "\n",
    "# Load the Quantized Lora model from the saved parameters (post-training)\n",
    "test_model = peft.PeftModel.from_pretrained(base_model, peft_model_name, is_trainable=False) #Flag: Inference\n",
    "test_model.print_trainable_parameters()\n",
    "\n",
    "# Here we can see that this trained model uses ~6.8B Params for base model + 33.6M Params for the adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d89dac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = peft.PeftModel.from_pretrained(base_model, peft_model_name, is_trainable=False) #Flag: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a70cd7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'peft.peft_model.PeftModelForCausalLM'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssharp/.local/lib/python3.9/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'shape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_129/1978905376.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# PEFT models:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.input_ids.to(\"cuda\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model_prepare_inputs_for_generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1306\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbos_token_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         )\n\u001b[0;32m-> 1308\u001b[0;31m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m         \u001b[0;31m# 4. Define other model kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##### Try to sample model outputs: #####\n",
    "\n",
    "print(type(test_model))\n",
    "# Sample messages to forward:\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who responds in pirate speak or in sea-shanties.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is your quest?\"},\n",
    "]\n",
    "\n",
    "sample_text = \"Can you sing me a sea shanty?\"\n",
    "\n",
    "# Causal models:\n",
    "#tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "#outputs = model.generate(**tokenized_prompt)\n",
    "# PEFT models:\n",
    "input_ids= tokenizer(sample_text, return_tensors=\"pt\")#.input_ids.to(\"cuda\")\n",
    "outputs = test_model.generate(input_ids=input_ids)\n",
    "tokenizer.decode(outputs)\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize input text\n",
    "#input_ids = tokenizer.encode(messages, return_tensors=\"pt\")\n",
    "#input_ids = test_model.tokenizer.encode(sample_text, return_tensors=\"pt\") \n",
    "\n",
    "\n",
    "# Generate output tokens\n",
    "#with torch.no_grad():\n",
    "    #output = test_model.generate(input_ids, max_length=8192, num_return_sequences=1)\n",
    "    #output = test_model.generate(input_ids, max_length=8192)\n",
    "\n",
    "# Decode output tokens into text\n",
    "#output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Print output text\n",
    "print(output_text)\n",
    "\n",
    "\n",
    "# # Formatting to send messages through tokenizer\n",
    "# input_ids = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     add_generation_prompt=True,\n",
    "#     return_tensors=\"pt\"\n",
    "# ).to(test_model.device)\n",
    "\n",
    "# terminators = [\n",
    "#     tokenizer.eos_token_id,\n",
    "#     tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "# ]\n",
    "\n",
    "# torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "# torch.backends.cuda.enable_flash_sdp(False)\n",
    "# # https://github.com/Lightning-AI/litgpt/issues/327\n",
    "\n",
    "# # Frame output formatting:\n",
    "# outputs = test_model.generate(\n",
    "#     input_ids,\n",
    "#     max_new_tokens=128,\n",
    "#     eos_token_id=terminators,\n",
    "#     do_sample=True,\n",
    "#     temperature=0.6,\n",
    "#     top_p=0.9,\n",
    "# )\n",
    "\n",
    "# # Push Sample Message through the model, display output\n",
    "# response = outputs[0][input_ids.shape[-1]:]\n",
    "# print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7a9d12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f862a24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage: 13205839872 bytes\n",
      "CUDA Memory allocated (bytes): 9032909824\n"
     ]
    }
   ],
   "source": [
    "# Try to measure memory used:\n",
    "#from transformers import GPT2LMHeadModel\n",
    "import psutil\n",
    "import torch\n",
    "#from torchsummary import summary\n",
    "\n",
    "# Need to find what \"input_size\" references here for an LLM\n",
    "#summary(test_model, input_size=(input_shape))\n",
    "\n",
    "\n",
    "# Get memory usage (bytes) \n",
    "memory_usage = psutil.Process().memory_info().rss\n",
    "print(\"Memory Usage:\", memory_usage, \"bytes\")\n",
    "# This is not useful, just returns available RAM being used by the alloted datahub kernel.\n",
    "\n",
    "# device isn't a hard/specific handler here.\n",
    "# (in Linux Terminal) nvidia-smi :doesn't refer consumed GPU since its not actively running while waiting.\n",
    "# need to devize a way to measure a GPU peak* consumption?\n",
    "\n",
    "#test_model.to(device=\"cuda\")\n",
    "#print(\"torch.cuda summary: \",torch.cuda.memory_summary(device=device_map))\n",
    "memory_allocated = torch.cuda.memory_allocated(device=\"cuda\")\n",
    "print(\"CUDA Memory allocated (bytes):\", memory_allocated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eebb092",
   "metadata": {},
   "source": [
    "#### Note: \n",
    "- Many wrappers are leading ambiguity -- unsure where only one wrapper around functional model, where many wrappers, or where wrappers are modified copies of other wrappers.\n",
    "\n",
    "- when loading model from parameters, the Datahub Kernel keeps dying (not consistently, but commonly) -- may be some form of memory issue or numerical instability. \n",
    "\n",
    "- our new, Quantized fine-tune model must be loaded alongside the original model. This does seem efficient to save space for those that try many fine-tunes for the same models. _This may be good to use to exploit larger models with special objectives_: could this be a common form MoE models use?\n",
    "\n",
    "- Model Params: [trainable params: 33,554,432 || all params: 6,771,970,048 || trainable%: 0.49548996469513035] This tells us that the model is uring 6.8B Parameters for the base model + 33.6M params for the adapter. This is useful information for our starting place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3420a5d",
   "metadata": {},
   "source": [
    "# Trial 2:\n",
    "To try to force a Llama-3-8B model into a Q4-quant for compliance to our memory constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1373f5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Repository Issues, but this should provide a good sample Format:\n",
    "# #Initial guide found in source: https://medium.com/@avishekpaul31/fine-tuning-llama-3-8b-instruct-qlora-using-low-cost-resources-89075e0dfa04\n",
    "# !pip install huggingface_hub\n",
    "\n",
    "# from huggingface_hub import notebook_login\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import torch\n",
    "\n",
    "# model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# # Sample messages to forward:\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak or Sea-shanties!\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "# ]\n",
    "\n",
    "# # Formatting to send messages through tokenizer\n",
    "# input_ids = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     add_generation_prompt=True,\n",
    "#     return_tensors=\"pt\"\n",
    "# ).to(model.device)\n",
    "\n",
    "# terminators = [\n",
    "#     tokenizer.eos_token_id,\n",
    "#     tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "# ]\n",
    "\n",
    "# torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "# torch.backends.cuda.enable_flash_sdp(False)\n",
    "# # https://github.com/Lightning-AI/litgpt/issues/327\n",
    "\n",
    "# # Frame output formatting:\n",
    "# outputs = model.generate(\n",
    "#     input_ids,\n",
    "#     max_new_tokens=128,\n",
    "#     eos_token_id=terminators,\n",
    "#     do_sample=True,\n",
    "#     temperature=0.6,\n",
    "#     top_p=0.9,\n",
    "# )\n",
    "\n",
    "# # Push Sample Message through the model, display output\n",
    "# response = outputs[0][input_ids.shape[-1]:]\n",
    "# print(tokenizer.decode(response, skip_special_tokens=True))\n",
    "\n",
    "#Doesnt run due to more repository compliance | compatibility issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8598fcff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
